{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Periodic padding in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in implementing a convolutional neural net which acts on periodic data.\n",
    "Unfortunately, periodic padding doesn't seem to be natively available in tensorflow or pytorch.\n",
    "This notebook will explore how to produce periodically-padded tensorflow convolutions with neutral, up, and down striding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete periodic convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the discrete convolution $y = k * x$, with a kernel of size P, centered on index $d$, and acting on data of size N, in index notation as\n",
    "\n",
    "$$\n",
    "y_i = \\sum_{p=0}^{P-1} k_{p} x_{(i+p-d) (\\mathrm{mod} N)}\n",
    "$$\n",
    "\n",
    "where the indeces are taken to be modulo N via the underlying periodicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be rewritten as a matrix multiplication $y = K \\cdot x$, or \n",
    "\n",
    "$$y_i = \\sum_{j=0}^{N-1} K_{ij} x_j$$\n",
    "\n",
    "where $K_{ij} = k_{(j-i+d) (\\mathrm{mod} N)}$ and $k_p = 0$ for $p \\ge P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a quick numerical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: [1 2 3]\n",
      "K matrix:\n",
      "[[2 3 0 0 0 0 0 1]\n",
      " [1 2 3 0 0 0 0 0]\n",
      " [0 1 2 3 0 0 0 0]\n",
      " [0 0 1 2 3 0 0 0]\n",
      " [0 0 0 1 2 3 0 0]\n",
      " [0 0 0 0 1 2 3 0]\n",
      " [0 0 0 0 0 1 2 3]\n",
      " [3 0 0 0 0 0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# Set sizes and build simple kernel\n",
    "N = 8\n",
    "P = 3\n",
    "d = 1\n",
    "k = np.arange(1, P+1)\n",
    "print('Kernel:', k)\n",
    "\n",
    "# Build K matrix (slow algorithm)\n",
    "K = np.zeros((N, N), dtype=int)\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        p = (j - i + d) % N\n",
    "        if p < P:\n",
    "            K[i, j] = k[p]\n",
    "            \n",
    "print('K matrix:')\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding to achieve periodicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tensorflow convolutions do not implement the periodic wrapping we need (i.e. indexing in modular arithmetic), we'll implement the same via periodically padding our input to size $N + P - 1$.\n",
    "\n",
    "We'll represent the padding operator as the linear operator $z = e(x, P-1)$, which in index notation is simply $\n",
    "z_i = x_{(i-d) (mod N)}$.\n",
    "This can again be rewritten as a matrix multiplication $z = E \\cdot x$ where $E$ is a a matrix with shape $(N+P-1, N)$ and $E_{ij} = \\delta_{(i-d) (\\mathrm{mod} N), j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a numerical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E matrix:\n",
      "[[0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0]]\n",
      "x: \n",
      "  [1 2 3 4 5 6 7 8]\n",
      "z = G @ x: \n",
      "  [8 1 2 3 4 5 6 7 8 1]\n"
     ]
    }
   ],
   "source": [
    "# Build extension matrix (slow algorithm)\n",
    "E = np.zeros((N+P-1, N), dtype=int)\n",
    "for i in range(N+P-1):\n",
    "    for j in range(N):\n",
    "        if (i-d) % N == j:\n",
    "            E[i, j] = 1\n",
    "            \n",
    "print('E matrix:')\n",
    "print(E)\n",
    "\n",
    "# Apple to an example\n",
    "x = np.arange(1, N+1)\n",
    "print('x: \\n ', x)\n",
    "z = E @ x\n",
    "print('z = G @ x: \\n ', z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a new convolution matrix which acts with \"valid\" padding on $z$ as $y = G \\cdot z$ where $G$ has shape $(N, N+P-1)$ and $G_{ij} = k_{j-i}$.\n",
    "\n",
    "We can verify the correctness of this formulion analytically:\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i &= \\sum_{j=0}^{N+P-2} G_{ij} z_j \\\\\n",
    "&= \\sum_{j=0}^{N+P-2} k_{j-i} x_{(j-d) (\\mathrm{mod} N)} \\\\\n",
    "&= \\sum_{p=-i}^{N+P-2-i} k_{p} x_{(i+p-d) (\\mathrm{mod} N)} \\\\\n",
    "&= \\sum_{p=0}^{P-1} k_{p} x_{(i+p-d) (\\mathrm{mod} N)} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G matrix:\n",
      "[[1 2 3 0 0 0 0 0 0 0]\n",
      " [0 1 2 3 0 0 0 0 0 0]\n",
      " [0 0 1 2 3 0 0 0 0 0]\n",
      " [0 0 0 1 2 3 0 0 0 0]\n",
      " [0 0 0 0 1 2 3 0 0 0]\n",
      " [0 0 0 0 0 1 2 3 0 0]\n",
      " [0 0 0 0 0 0 1 2 3 0]\n",
      " [0 0 0 0 0 0 0 1 2 3]]\n",
      "y = K @ x: \n",
      "  [16 14 20 26 32 38 44 26]\n",
      "y = G @ E @ x: \n",
      "  [16 14 20 26 32 38 44 26]\n",
      "  Same: True\n"
     ]
    }
   ],
   "source": [
    "# Build extended convolution matrix (slow algorithm)\n",
    "G = np.zeros((N, N+P-1), dtype=int)\n",
    "for i in range(N):\n",
    "    for j in range(N+P-1):\n",
    "        p = j - i\n",
    "        if 0 <= p < P:\n",
    "            G[i, j] = k[p]\n",
    "            \n",
    "print('G matrix:')\n",
    "print(G)\n",
    "\n",
    "# Test reformulation\n",
    "y0 = K @ x\n",
    "print('y = K @ x: \\n ', y0)\n",
    "y1 = G @ z\n",
    "print('y = G @ E @ x: \\n ', y1)\n",
    "print('  Same:', np.allclose(y0, y1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's give it a try in tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = (tf conv): \n",
      "  [16. 14. 20. 26. 32. 38. 44. 26.]\n",
      "  Same: True\n"
     ]
    }
   ],
   "source": [
    "# Reshape input as (batch, *size, channels)\n",
    "z_tens = tf.convert_to_tensor(z[None, None, :, None].astype(float))\n",
    "# Reshape kernel as (*size, channels, features)\n",
    "k_tens = tf.convert_to_tensor(k[None, :, None, None].astype(float))\n",
    "\n",
    "y2_tens = keras.backend.conv2d(z_tens, k_tens, padding='valid')\n",
    "y2 = y2_tens.numpy().flatten()\n",
    "\n",
    "print('y = (tf conv): \\n ', y2)\n",
    "print('  Same:', np.allclose(y0, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling via striding is equivalent to simply striding the output, or striding the rows of $K$ or $G$, which we can write as $S \\cdot K = S \\cdot G \\cdot E$.\n",
    "The tensorflow convolutions should do the same, even when acting on the padded inputs, since they've been padded just enough to properly convolve the original input.\n",
    "\n",
    "Let's check this numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = (strided tf conv): \n",
      "  [16. 32.]\n",
      "  Same: True\n"
     ]
    }
   ],
   "source": [
    "# Set stride size\n",
    "s = 4\n",
    "\n",
    "# Run strided convolution\n",
    "y3_tens = keras.backend.conv2d(z_tens, k_tens, padding='valid', strides=(1,s))\n",
    "y3 = y3_tens.numpy().flatten()\n",
    "\n",
    "print('y = (strided tf conv): \\n ', y3)\n",
    "print('  Same:', np.allclose(y0[::s], y3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsampling via striding is equivalent to transposing the strided $K$, i.e. $(S \\cdot K)^T = E^T \\cdot (S \\cdot G)^T$.\n",
    "The second operator $(S \\cdot G)^T$ is implemented via the strided transpose convolutions in tensorflow, so we can produce the full convolution by applying $E.T$ to the output of this layer.\n",
    "\n",
    "Let's check this numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = K[::s].T @ y: \n",
      "  [2 3 0 2 4 6 0 1]\n",
      "x = E.T @ G[::s].T @ y: \n",
      "  [2 3 0 2 4 6 0 1]\n",
      "  Same: True\n",
      "x = E.T @ (tf conv trans): \n",
      "  [2. 3. 0. 2. 4. 6. 0. 1.]\n",
      "  Same: True\n"
     ]
    }
   ],
   "source": [
    "# Produce input data\n",
    "M = (N + s - 1) // s\n",
    "y = np.arange(1, M+1)\n",
    "\n",
    "# Direct up-convolutions\n",
    "x0 = K[::s].T @ y\n",
    "print('x = K[::s].T @ y: \\n ', x0)\n",
    "x1 = E.T @ G[::s].T @ y\n",
    "print('x = E.T @ G[::s].T @ y: \\n ', x1)\n",
    "print('  Same:', np.allclose(x0, x1))\n",
    "\n",
    "# Reshape input as (batch, *size, channels)\n",
    "y_tens = tf.convert_to_tensor(y[None, None, :, None].astype(float))\n",
    "\n",
    "# Run strided up-convolution\n",
    "output_shape = (z_tens.shape)\n",
    "x2_tens = keras.backend.conv2d_transpose(y_tens, k_tens, output_shape, padding='valid', strides=(1,s))\n",
    "x2 = E.T @ x2_tens.numpy().flatten()\n",
    "\n",
    "print('x = E.T @ (tf conv trans): \\n ', x2)\n",
    "print('  Same:', np.allclose(x0, x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We have seen that we simply need to have implementations of the forward and transposed padding/extension operators $E$ and $E^T$ to produce periodic convolutions with tensorflow.\n",
    "These two operators need to be fast and differentiable to be usable in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's also check our custom tensorflow implementations for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equisampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = (tf conv): \n",
      "  [16. 14. 20. 26. 32. 38. 44. 26.]\n",
      "  Same: True\n"
     ]
    }
   ],
   "source": [
    "# Reshape input as (batch, *size, channels)\n",
    "x3_tens = tf.convert_to_tensor(x[None, None, None, :, None].astype(float))\n",
    "# Reshape kernel as (*size, channels, features)\n",
    "k3_tens = tf.convert_to_tensor(k[None, None, :, None, None].astype(float))\n",
    "\n",
    "filters = 1\n",
    "kernel_size = (1, 1, P)\n",
    "kernel_center = (0, 0, d)\n",
    "input_shape = (1, 1, 1, N, 1)\n",
    "dtype = np.float64\n",
    "weights = [k3_tens, np.array([0])]\n",
    "\n",
    "model = unet.PeriodicConv3D(filters, kernel_size, kernel_center)\n",
    "model(tf.zeros(input_shape, dtype=dtype))\n",
    "model.set_weights(weights)\n",
    "y4 = model(x3_tens).numpy().flatten()\n",
    "\n",
    "print('y = (tf conv): \\n ', y4)\n",
    "print('  Same:', np.allclose(y0, y4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = (strided tf conv): \n",
      "  [16. 32.]\n",
      "  Same: True\n"
     ]
    }
   ],
   "source": [
    "strides = (1, 1, s)\n",
    "\n",
    "model = unet.PeriodicConv3D(filters, kernel_size, kernel_center, strides=strides)\n",
    "model(tf.zeros(input_shape, dtype=dtype))\n",
    "model.set_weights(weights)\n",
    "y5 = model(x3_tens).numpy().flatten()\n",
    "\n",
    "print('y = (strided tf conv): \\n ', y5)\n",
    "print('  Same:', np.allclose(y0[::s], y5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = E.T @ (tf conv trans): \n",
      "  [1. 2. 3. 0. 2. 4. 6. 0.]\n",
      "[2 3 0 2 4 6 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Reshape input as (batch, *size, channels)\n",
    "y3_tens = tf.convert_to_tensor(y[None, None, None, :, None].astype(float))\n",
    "\n",
    "#model = unet.PeriodicConv3DTranspose(filters, kernel_size, kernel_center, strides=strides)\n",
    "model = keras.layers.Conv2DTranspose(filters, kernel_size[1:], strides=strides[1:], padding='valid')\n",
    "model(tf.zeros(y3_tens.shape[1:], dtype=dtype))\n",
    "model.set_weights([k3_tens[0], np.array([0])])\n",
    "x3 = model(y3_tens[0]).numpy().flatten()\n",
    "\n",
    "print('x = E.T @ (tf conv trans): \\n ', x3)\n",
    "#print('  Same:', np.allclose(x0, x3))\n",
    "print(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
